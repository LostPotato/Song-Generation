{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJnIpDlXPo_D"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "This will be an example of using the KERAS to do text generation at a character level\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ofz3B4WmuJZ",
        "outputId": "d0c88b46-c77b-4fd5-f43a-9045cbfb55be"
      },
      "source": [
        "# importing all of the libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import np_utils\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# grabbing the text and lowering each one\n",
        "text=(open(\"/content/drive/MyDrive/NLP Notes/Mix.txt\").read())\n",
        "\n",
        "text=text.lower()\n",
        "\n",
        "# creating a dictonary of each alpha numeric text occurence in the soonet: \n",
        "words = sorted(list(set(text)))\n",
        "# these are feature dictonaries, they hold the index location for each of them\n",
        "n_to_words = {n:char for n, char in enumerate(words)}\n",
        "words_to_n = {char:n for n, char in enumerate(words)}\n",
        "\n",
        "# showing what these look like when you\n",
        "for key in n_to_words:\n",
        "    print(key, n_to_words[key])\n",
        "for key in words_to_n:\n",
        "    print(key,words_to_n[key])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "0 \n",
            "\n",
            "1  \n",
            "2 !\n",
            "3 \"\n",
            "4 $\n",
            "5 &\n",
            "6 '\n",
            "7 (\n",
            "8 )\n",
            "9 ,\n",
            "10 -\n",
            "11 .\n",
            "12 0\n",
            "13 1\n",
            "14 2\n",
            "15 3\n",
            "16 4\n",
            "17 5\n",
            "18 7\n",
            "19 8\n",
            "20 9\n",
            "21 :\n",
            "22 ;\n",
            "23 ?\n",
            "24 [\n",
            "25 ]\n",
            "26 a\n",
            "27 b\n",
            "28 c\n",
            "29 d\n",
            "30 e\n",
            "31 f\n",
            "32 g\n",
            "33 h\n",
            "34 i\n",
            "35 j\n",
            "36 k\n",
            "37 l\n",
            "38 m\n",
            "39 n\n",
            "40 o\n",
            "41 p\n",
            "42 q\n",
            "43 r\n",
            "44 s\n",
            "45 t\n",
            "46 u\n",
            "47 v\n",
            "48 w\n",
            "49 x\n",
            "50 y\n",
            "51 z\n",
            "52 ¡\n",
            "53 ç\n",
            "54 î\n",
            "55 ó\n",
            "56 ’\n",
            "\n",
            " 0\n",
            "  1\n",
            "! 2\n",
            "\" 3\n",
            "$ 4\n",
            "& 5\n",
            "' 6\n",
            "( 7\n",
            ") 8\n",
            ", 9\n",
            "- 10\n",
            ". 11\n",
            "0 12\n",
            "1 13\n",
            "2 14\n",
            "3 15\n",
            "4 16\n",
            "5 17\n",
            "7 18\n",
            "8 19\n",
            "9 20\n",
            ": 21\n",
            "; 22\n",
            "? 23\n",
            "[ 24\n",
            "] 25\n",
            "a 26\n",
            "b 27\n",
            "c 28\n",
            "d 29\n",
            "e 30\n",
            "f 31\n",
            "g 32\n",
            "h 33\n",
            "i 34\n",
            "j 35\n",
            "k 36\n",
            "l 37\n",
            "m 38\n",
            "n 39\n",
            "o 40\n",
            "p 41\n",
            "q 42\n",
            "r 43\n",
            "s 44\n",
            "t 45\n",
            "u 46\n",
            "v 47\n",
            "w 48\n",
            "x 49\n",
            "y 50\n",
            "z 51\n",
            "¡ 52\n",
            "ç 53\n",
            "î 54\n",
            "ó 55\n",
            "’ 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOmyqeUrg9i"
      },
      "source": [
        "\n",
        "\n",
        "#Understanding how the data is being set up:\n",
        "\n",
        "The next step is to prepare how the data will be fed into the LSTM and how you to hold the output.\n",
        "--\n",
        "**First** -- we establish a context that we want the model to look at. This is our look up length, basically its how many examples of letters do we have before to predict the next letter\n",
        "-- \n",
        "**Second** -- we make a reference for every single character in the text, THESE are huge arrays but provide the LSTM with 100 examples for each character till the end of our document\n",
        "-- \n",
        "**Last** -- we provide the true values in the target array, i.e what the letter should be given the last 100 examples it had\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47oDFvE8XhoO"
      },
      "source": [
        "# setting up the arrays to hold data\n",
        "train_array = []\n",
        "target_array = []\n",
        "\n",
        "# getting the length of the text document\n",
        "length = len(text)\n",
        "\n",
        "# Setting up how many characters we want the model to look up for context\n",
        "look_up_length = 50\n",
        "\n",
        "# creating a look-up table so that the model understands the context around the character its looking at\n",
        "for i in range(0, length-look_up_length, 1):\n",
        "    # creating the squence of characters for each context window\n",
        "    sequence = text[i:i+look_up_length]\n",
        "    # creating a label to reference each character in the sequence\n",
        "    labels = text[i+look_up_length]\n",
        "    # appending the sqeunce of characters to the training array\n",
        "    train_array.append([words_to_n[char]for char in sequence])\n",
        "    # adding the label of what character should come next for each sequence\n",
        "    target_array.append(words_to_n[labels])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUcQ86KjbMoD"
      },
      "source": [
        "The next step is to transform the data so that our model can actually understand it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSXgkRHKbp5S",
        "outputId": "2e75677c-ad70-44bb-8de8-1302898ce8c4"
      },
      "source": [
        "# making the training array into 3-D\n",
        "modded_training = np.reshape(train_array, (len(train_array), look_up_length, 1))\n",
        "# normalizing by the modifed training data\n",
        "final_mod_training = modded_training / float(len(words)) \n",
        "# Modding the target array to remove relationships of of distance within the array \n",
        "final_mod_target = np_utils.to_categorical(target_array)\n",
        "\n",
        "# giving a idea of what these look like\n",
        "print(final_mod_target[1] )\n",
        "print(final_mod_training[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[[0.68421053]\n",
            " [0.49122807]\n",
            " [0.52631579]\n",
            " [0.01754386]\n",
            " [0.80701754]\n",
            " [0.71929825]\n",
            " [0.70175439]\n",
            " [0.68421053]\n",
            " [0.01754386]\n",
            " [0.45614035]\n",
            " [0.01754386]\n",
            " [0.78947368]\n",
            " [0.59649123]\n",
            " [0.66666667]\n",
            " [0.52631579]\n",
            " [0.15789474]\n",
            " [0.01754386]\n",
            " [0.59649123]\n",
            " [0.01754386]\n",
            " [0.77192982]\n",
            " [0.78947368]\n",
            " [0.59649123]\n",
            " [0.64912281]\n",
            " [0.64912281]\n",
            " [0.01754386]\n",
            " [0.47368421]\n",
            " [0.52631579]\n",
            " [0.64912281]\n",
            " [0.59649123]\n",
            " [0.52631579]\n",
            " [0.8245614 ]\n",
            " [0.52631579]\n",
            " [0.50877193]\n",
            " [0.01754386]\n",
            " [0.59649123]\n",
            " [0.68421053]\n",
            " [0.01754386]\n",
            " [0.66666667]\n",
            " [0.87719298]\n",
            " [0.77192982]\n",
            " [0.52631579]\n",
            " [0.64912281]\n",
            " [0.54385965]\n",
            " [0.        ]\n",
            " [0.59649123]\n",
            " [0.01754386]\n",
            " [0.84210526]\n",
            " [0.45614035]\n",
            " [0.77192982]\n",
            " [0.01754386]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQuta_-Vhlie"
      },
      "source": [
        "Experimental model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWnMU-Vyho2o"
      },
      "source": [
        "from tensorflow import keras\n",
        "# defining the model as sequential\n",
        "model = Sequential()\n",
        "# adding the inputs from the training data to a LSTM model\n",
        "model.add(LSTM(500, input_shape=(final_mod_training.shape[1], final_mod_training.shape[2]), return_sequences=True))\n",
        "# adding a drop out layer\n",
        "model.add(Dropout(0.3))\n",
        "# adding our second LSTM model\n",
        "model.add(LSTM(500, return_sequences = True))\n",
        "# adding a second drop out layer\n",
        "model.add(Dropout(0.3))\n",
        "# adding  a third layer\n",
        "model.add(LSTM(500))\n",
        "# adding a third drop out layer\n",
        "model.add(Dropout(0.3))\n",
        "# adding a dense layer\n",
        "model.add(Dense(final_mod_target.shape[1], activation='softmax'))\n",
        "# compling the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkZD9rk2wir"
      },
      "source": [
        "Viewing a summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2CqMha42v99",
        "outputId": "e17a0a6a-595a-449a-85eb-8cef1dcde0c6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 50, 500)           1004000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50, 500)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50, 500)           2002000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50, 500)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 500)               2002000   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 57)                28557     \n",
            "=================================================================\n",
            "Total params: 5,036,557\n",
            "Trainable params: 5,036,557\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiw55bNqkTqb"
      },
      "source": [
        "Traing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqQcCM0PkWFm",
        "outputId": "6f2e576a-ff94-474c-c693-3a8471883afc"
      },
      "source": [
        "model.fit(final_mod_training,final_mod_target, epochs=100, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3391/3391 [==============================] - 141s 41ms/step - loss: 2.7531\n",
            "Epoch 2/100\n",
            "3391/3391 [==============================] - 146s 43ms/step - loss: 2.3921\n",
            "Epoch 3/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 2.1463\n",
            "Epoch 4/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 1.9975\n",
            "Epoch 5/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.8914\n",
            "Epoch 6/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.8113\n",
            "Epoch 7/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.7432\n",
            "Epoch 8/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.6805\n",
            "Epoch 9/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.6247\n",
            "Epoch 10/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.5729\n",
            "Epoch 11/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.5265\n",
            "Epoch 12/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.4848\n",
            "Epoch 13/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.4422\n",
            "Epoch 14/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.4068\n",
            "Epoch 15/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.3748\n",
            "Epoch 16/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.3413\n",
            "Epoch 17/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.3094\n",
            "Epoch 18/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.2833\n",
            "Epoch 19/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.2589\n",
            "Epoch 20/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.2334\n",
            "Epoch 21/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.2113\n",
            "Epoch 22/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.1906\n",
            "Epoch 23/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 1.1709\n",
            "Epoch 24/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.1538\n",
            "Epoch 25/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.1365\n",
            "Epoch 26/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.1189\n",
            "Epoch 27/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.1019\n",
            "Epoch 28/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0877\n",
            "Epoch 29/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 1.0747\n",
            "Epoch 30/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0612\n",
            "Epoch 31/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0488\n",
            "Epoch 32/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0367\n",
            "Epoch 33/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0269\n",
            "Epoch 34/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0149\n",
            "Epoch 35/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0091\n",
            "Epoch 36/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.9950\n",
            "Epoch 37/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9860\n",
            "Epoch 38/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.9784\n",
            "Epoch 39/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.9660\n",
            "Epoch 40/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9627\n",
            "Epoch 41/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9578\n",
            "Epoch 42/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9490\n",
            "Epoch 43/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9369\n",
            "Epoch 44/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9368\n",
            "Epoch 45/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9251\n",
            "Epoch 46/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9246\n",
            "Epoch 47/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9176\n",
            "Epoch 48/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.9158\n",
            "Epoch 49/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9044\n",
            "Epoch 50/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9021\n",
            "Epoch 51/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8944\n",
            "Epoch 52/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8878\n",
            "Epoch 53/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8874\n",
            "Epoch 54/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8769\n",
            "Epoch 55/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8738\n",
            "Epoch 56/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8718\n",
            "Epoch 57/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8685\n",
            "Epoch 58/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8601\n",
            "Epoch 59/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8585\n",
            "Epoch 60/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.0971\n",
            "Epoch 61/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 2.7380\n",
            "Epoch 62/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9704\n",
            "Epoch 63/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8869\n",
            "Epoch 64/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8665\n",
            "Epoch 65/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8571\n",
            "Epoch 66/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8479\n",
            "Epoch 67/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8479\n",
            "Epoch 68/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8414\n",
            "Epoch 69/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8316\n",
            "Epoch 70/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8340\n",
            "Epoch 71/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8256\n",
            "Epoch 72/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8210\n",
            "Epoch 73/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8211\n",
            "Epoch 74/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8154\n",
            "Epoch 75/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8098\n",
            "Epoch 76/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8096\n",
            "Epoch 77/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8100\n",
            "Epoch 78/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 1.3515\n",
            "Epoch 79/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 2.5775\n",
            "Epoch 80/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 2.3152\n",
            "Epoch 81/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 2.3296\n",
            "Epoch 82/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 1.1803\n",
            "Epoch 83/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8931\n",
            "Epoch 84/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8618\n",
            "Epoch 85/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8456\n",
            "Epoch 86/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 1.4860\n",
            "Epoch 87/100\n",
            "3391/3391 [==============================] - 147s 43ms/step - loss: 2.5204\n",
            "Epoch 88/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 2.5232\n",
            "Epoch 89/100\n",
            "3391/3391 [==============================] - 147s 43ms/step - loss: 2.5281\n",
            "Epoch 90/100\n",
            "3391/3391 [==============================] - 147s 43ms/step - loss: 2.5235\n",
            "Epoch 91/100\n",
            "3391/3391 [==============================] - 147s 43ms/step - loss: 2.8202\n",
            "Epoch 92/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 1.2485\n",
            "Epoch 93/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.9623\n",
            "Epoch 94/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.9175\n",
            "Epoch 95/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8943\n",
            "Epoch 96/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8835\n",
            "Epoch 97/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8876\n",
            "Epoch 98/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8725\n",
            "Epoch 99/100\n",
            "3391/3391 [==============================] - 149s 44ms/step - loss: 0.8813\n",
            "Epoch 100/100\n",
            "3391/3391 [==============================] - 148s 44ms/step - loss: 0.8773\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7fc019ffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYM0FKbo83TL"
      },
      "source": [
        "Original Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9ci4dSW4zhI",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "from tensorflow import keras\n",
        "# defining the model as sequential\n",
        "model = Sequential()\n",
        "# adding the inputs from the training data to a LSTM model\n",
        "model.add(LSTM(500, input_shape=(final_mod_training.shape[1], final_mod_training.shape[2]), return_sequences=True))\n",
        "# adding a drop out layer\n",
        "model.add(Dropout(0.4))\n",
        "# adding our second LSTM model\n",
        "model.add(LSTM(500, return_sequences = True))\n",
        "# adding a second drop out layer\n",
        "model.add(Dropout(0.4))\n",
        "# adding  a third layer\n",
        "model.add(LSTM(500))\n",
        "# adding a third drop out layer\n",
        "model.add(Dropout(0.4))\n",
        "# adding a dense layer\n",
        "model.add(Dense(final_mod_target.shape[1], activation='softmax'))\n",
        "# setting up the optimizer\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0124)\n",
        "# compling the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_iarAtEwPq"
      },
      "source": [
        "# CODE BELOW NEEDS SOME WORK SON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDhjq21wGRcD"
      },
      "source": [
        "model.load_weights('/content/drive/MyDrive/Colab Notebooks/text_generator_giganticv2_65epocs.h5')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGzZIauAlBNk",
        "outputId": "d7a53c48-6307-4cc3-86bd-05ecbc79b11f"
      },
      "source": [
        "import random as rnd\n",
        "# having the system pick a random training sentence\n",
        "input_string  = rnd.choice(train_array) \n",
        "# converting the string into something the computer likes\n",
        "full_string = [n_to_words[value] for value in input_string]\n",
        "# the phrase the model is going off of\n",
        "lyrics = ''\n",
        "for i in full_string:\n",
        "    lyrics += i\n",
        "print(lyrics)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "es, lotta folds, i'm still tryin' to just get back\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW3Cv2CWpKFV"
      },
      "source": [
        "# Getting it generate!\n",
        "\n",
        "The model really does need some work, but I think the main thing\n",
        "is that the dataset just needs to be a lot bigger to get a better generation\n",
        "right now there is only about 70 or 80 songs in the test file, but I actually like the jank-ness of some the lyrics it generates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ttxdGW9qrnV",
        "outputId": "ae36d5cb-686b-45c4-f375-c7228984a7e6"
      },
      "source": [
        "# Setting it equal to the input string generated above\n",
        "string_mapped=input_string\n",
        "full_string = [n_to_words[value] for value in string_mapped]\n",
        "\n",
        "# Giving the string the values the model can use\n",
        "lyrics = ''\n",
        "for i in full_string:\n",
        "    lyrics += i\n",
        "print(lyrics)\n",
        "\n",
        "\n",
        "# generating characters\n",
        "for i in range(250):\n",
        "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
        "    x = x / float(len(words))\n",
        "\n",
        "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
        "    seq = [n_to_words[value] for value in string_mapped]\n",
        "    full_string.append(n_to_words[pred_index])\n",
        "\n",
        "    string_mapped.append(pred_index)\n",
        "    string_mapped = string_mapped[1:len(string_mapped)]\n",
        "\n",
        "# Printing the outcome of the models predictions\n",
        "txt=\"\"\n",
        "for char in full_string:\n",
        "    txt = txt+char\n",
        "print(txt)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "es, lotta folds, i'm still tryin' to just get back\n",
            "es, lotta folds, i'm still tryin' to just get back up\n",
            "\n",
            "i don't know what that means, but it's something to me that\n",
            "you make me feel better\n",
            "i don't gotta put you that you know you gotta lake you cringe\n",
            "\n",
            "i got the day i go insane\n",
            "that'll be the day, today might be the day i go insane\n",
            "the day i go insa\n"
          ]
        }
      ]
    }
  ]
}